{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "# import tensorflow_probability as tfp\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Flatten, concatenate, dot, Add, Dropout, BatchNormalization, \\\n",
    "    Activation, LeakyReLU, Lambda, Multiply,Conv1D,Softmax,Conv2D\n",
    "\n",
    "# import tensorflow_probability as tfp\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, Adagrad, Adadelta, SGD, Nadam\n",
    "from tensorflow.keras.layers import Conv1D as cnn1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "gpu_list = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpu_list) > 0 :\n",
    "    for gpu in gpu_list:\n",
    "        try:\n",
    "        # 设置多张 GPU ，如果不需要 for 去设置多张，则使用 list 的索引设置即可\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "else:\n",
    "    print(\"Got no GPUs\")\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras.engine.base_layer import Layer\n",
    "\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, Multiply\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "\n",
    "EMBED_DIM = 512  # Latent dimension\n",
    "\n",
    "# Embedding helper\n",
    "def get_embedding(input_dim, output_dim, input_length, name):\n",
    "    return Embedding(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        input_length=input_length,\n",
    "        embeddings_initializer=RandomNormal(),\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "# Multi-head self-attention block\n",
    "def apply_attention(inputs):\n",
    "    attn = MultiHeadAttention(num_heads=2, key_dim=EMBED_DIM)(inputs, inputs)\n",
    "    return Dense(EMBED_DIM, activation='relu')(attn)\n",
    "\n",
    "# Main R2SL model\n",
    "def create_r2sl_model():\n",
    "    # Input definition\n",
    "    inputs = {\n",
    "        'user_id': Input(shape=(1,), dtype='int64', name='user_id_input'),\n",
    "        'user_region': Input(shape=(2,), dtype='int64', name='user_lc_input'),\n",
    "        'item_id': Input(shape=(1,), dtype='int64', name='item_id_input'),\n",
    "        'item_region': Input(shape=(2,), dtype='int64', name='item_lc_input'),\n",
    "        'user_id_2': Input(shape=(1,), dtype='int64', name='user_id_2_input'),\n",
    "        'user_region_2': Input(shape=(2,), dtype='int64', name='user_lc_2_input'),\n",
    "        'item_id_2': Input(shape=(1,), dtype='int64', name='item_id_2_input'),\n",
    "        'item_region_2': Input(shape=(2,), dtype='int64', name='item_lc_2_input'),\n",
    "    }\n",
    "\n",
    "    # Embedding layers (shared across related inputs)\n",
    "    emb_user_id = get_embedding(339, EMBED_DIM, 1, 'user_id_embedding')\n",
    "    emb_user_reg = get_embedding(339, EMBED_DIM, 2, 'user_region_embedding')\n",
    "    emb_item_id = get_embedding(5825, EMBED_DIM, 1, 'item_id_embedding')\n",
    "    emb_item_reg = get_embedding(5825, EMBED_DIM, 2, 'item_region_embedding')\n",
    "\n",
    "    # Apply embedding + flatten\n",
    "    def embed_and_flatten(layer, name):\n",
    "        return Flatten(name=name+'_flat')(layer)\n",
    "\n",
    "    # Embedding lookups\n",
    "    user_id_latent = embed_and_flatten(emb_user_id(inputs['user_id']), 'user_id')\n",
    "    user_reg_latent = embed_and_flatten(emb_user_reg(inputs['user_region']), 'user_region')\n",
    "    item_id_latent = embed_and_flatten(emb_item_id(inputs['item_id']), 'item_id')\n",
    "    item_reg_latent = embed_and_flatten(emb_item_reg(inputs['item_region']), 'item_region')\n",
    "\n",
    "    user_id_2_latent = embed_and_flatten(emb_user_id(inputs['user_id_2']), 'user_id_2')\n",
    "    user_reg_2_latent = embed_and_flatten(emb_user_reg(inputs['user_region_2']), 'user_region_2')\n",
    "    item_id_2_latent = embed_and_flatten(emb_item_id(inputs['item_id_2']), 'item_id_2')\n",
    "    item_reg_2_latent = embed_and_flatten(emb_item_reg(inputs['item_region_2']), 'item_region_2')\n",
    "\n",
    "    # Feature concatenation\n",
    "    f1 = Concatenate()([user_id_latent, user_reg_latent, item_id_latent, item_reg_latent])\n",
    "    f2 = Concatenate()([user_id_2_latent, user_reg_2_latent, item_id_2_latent, item_reg_2_latent])\n",
    "    region_as = Concatenate()([user_reg_latent, user_reg_2_latent])\n",
    "    region_city = Concatenate()([item_reg_latent, item_reg_2_latent])\n",
    "\n",
    "    # Shared towers\n",
    "    def dense_tower(x, name_prefix):\n",
    "        x = Dense(512, activation='relu', name=f'{name_prefix}_1')(x)\n",
    "        x = Dense(256, activation='relu', name=f'{name_prefix}_2')(x)\n",
    "        return Dense(128, activation='relu', name=f'{name_prefix}_3')(x)\n",
    "\n",
    "    tower_f1 = dense_tower(f1, 'tower_f1')\n",
    "    tower_f2 = dense_tower(f2, 'tower_f2')\n",
    "    tower_as = dense_tower(region_as, 'tower_as')\n",
    "    tower_city = dense_tower(region_city, 'tower_city')\n",
    "\n",
    "    # Gating: user_id_latent and user_id_2_latent control two gate flows\n",
    "    def gated_merge(domain_vec, expert_vec, name_prefix):\n",
    "        g = Dense(512, activation='relu', name=f'{name_prefix}_gate_1')(domain_vec)\n",
    "        g = Dense(256, activation='relu', name=f'{name_prefix}_gate_2')(g)\n",
    "        g = Dense(1, activation='sigmoid', name=f'{name_prefix}_gate_sigmoid')(g)\n",
    "        return Multiply(name=f'{name_prefix}_gated')([expert_vec, g])\n",
    "\n",
    "    merged1 = gated_merge(user_id_latent, tower_as, 'merge1')\n",
    "    merged2 = gated_merge(user_id_latent, tower_city, 'merge2')\n",
    "    merged3 = gated_merge(user_id_2_latent, tower_as, 'merge3')\n",
    "    merged4 = gated_merge(user_id_2_latent, tower_city, 'merge4')\n",
    "\n",
    "    # Attention-enhanced fusion\n",
    "    fusion_rt = Concatenate(name='fusion_rt')([tower_f1, merged1, merged2])\n",
    "    fusion_tp = Concatenate(name='fusion_tp')([tower_f2, merged3, merged4])\n",
    "\n",
    "    attn_rt = apply_attention(tf.expand_dims(fusion_rt, axis=1))\n",
    "    attn_tp = apply_attention(tf.expand_dims(fusion_tp, axis=1))\n",
    "\n",
    "    # Prediction heads\n",
    "    def output_head(x, base, task_name):\n",
    "        x = Dense(2048, activation='relu', name=f'{task_name}_fc1')(x)\n",
    "        x = Dense(1024, activation='relu', name=f'{task_name}_fc2')(Concatenate()([x, base]))\n",
    "        x = Dense(512, activation='relu', name=f'{task_name}_fc3')(x)\n",
    "        return Dense(1, activation='linear', name=task_name)(x)\n",
    "\n",
    "    rt_output = output_head(attn_rt, tower_f1, 'rt_output')\n",
    "    tp_output = output_head(attn_tp, tower_f2, 'tp_output')\n",
    "\n",
    "    model = Model(inputs=list(inputs.values()), outputs=[rt_output, tp_output])\n",
    "    return model\n",
    "\n",
    "# Instantiate and compile the model\n",
    "model = create_r2sl_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'rt_output': 'mae', 'tp_output': 'mae'},\n",
    "              metrics={'rt_output': 'mae', 'tp_output': 'mae'})\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "df_rt = pd.read_csv('./Data/WSDream/Dataset#1/rt_train_0.075.txt', sep='\\t')\n",
    "df_tp = pd.read_csv('./Data/WSDream/Dataset#1/tp_train_0.075.txt', sep='\\t')\n",
    "df_tp = df_tp.rename(columns={col: col + \"_2\" if col != \"TP\" else col for col in df_tp.columns})\n",
    "\n",
    "\n",
    "df = pd.concat([df_rt, df_tp], axis=1)\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "rt_labels = df['[RT]'].values\n",
    "tp_labels = df['[TP]_2'].values\n",
    "x_train=[df[['[User ID]']],df[['[User AS]','[User Country]']],df[['[Service ID]']],df[['[Service AS]','[Service Country]']],df[['[User ID]_2']],df[['[User AS]_2','[User Country]_2']],df[['[Service ID]_2']],df[['[Service AS]_2','[Service Country]_2']]]\n",
    "\n",
    "\n",
    "df_test_rt = pd.read_csv('./Data/WSDream/Dataset#1/rt_test_0.075.txt', sep='\\t')\n",
    "df_test_tp = pd.read_csv('./Data/WSDream/Dataset#1/tp_test_0.075.txt', sep='\\t')\n",
    "df_test_tp = df_test_tp.rename(columns={col: col + \"_2\" if col != \"TP\" else col for col in df_test_tp.columns})\n",
    "\n",
    "\n",
    "df_test = pd.concat([df_test_rt, df_test_tp], axis=1)\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "\n",
    "rt_test_labels = df_test['[RT]'].values\n",
    "tp_test_labels = df_test['[TP]_2'].values\n",
    "\n",
    "x_test=[df_test[['[User ID]']],df_test[['[User AS]','[User Country]']],df_test[['[Service ID]']],df_test[['[Service AS]','[Service Country]']],df_test[['[User ID]_2']],df_test[['[User AS]_2','[User Country]_2']],df_test[['[Service ID]_2']],df_test[['[Service AS]_2','[Service Country]_2']]]\n",
    "\n",
    "\n",
    "# compile autoencoder\n",
    "# autoencoder.compile(optimizer='adam', loss=huber_loss,metrics=['mae'])\n",
    "# from Evaluator import evaluate, saveResult\n",
    "\n",
    "autoencoder = create_multi_task_model()\n",
    "autoencoder.compile(optimizer='Nadam', \n",
    "              loss={'rt_output': 'mae', 'tp_output': 'mae'}, \n",
    "        #        loss={'rt_output': 'mae'}, \n",
    "              loss_weights={'rt_output': 1., 'tp_output':1.},\n",
    "              metrics={'rt_output': 'mae', 'tp_output': 'mae'})\n",
    "import sys\n",
    "from time import time\n",
    "evalResults = np.zeros((30, 3))\n",
    "sys.stdout.write('\\rInitializing...')\n",
    "rt_test, tp_rest = autoencoder.predict(x_test, batch_size=64)\n",
    "mae = mean_absolute_error(rt_test_labels, rt_test)\n",
    "rmse = np.sqrt(mean_squared_error(rt_test_labels, rt_test))\n",
    "mae_tp = mean_absolute_error(tp_test_labels, tp_rest)\n",
    "rmse_tp = np.sqrt(mean_squared_error(tp_test_labels, tp_rest))\n",
    "\n",
    "sys.stdout.write('\\rInitializing completes.MAE = %.4f|RMSE = %.4f.\\n' % (mae, rmse))\n",
    "best_mae, best_rmse, best_epoch = mae, rmse, -1\n",
    "for epoch in range(30):\n",
    "        sys.stdout.write('\\rEpoch %d starts...' % epoch)\n",
    "        start = time()\n",
    "        # Training\n",
    "        history = autoencoder.fit(x_train, {'rt_output': rt_labels, 'tp_output': tp_labels}, batch_size=64, epochs=1, verbose=0, shuffle=True)\n",
    "        # , callbacks=[TensorBoard(log_dir='./Log')])\n",
    "        end = time()\n",
    "        sys.stdout.write('\\rEpoch %d ends.[%.1fs]' % (epoch, end - start))\n",
    "        # print(end - start)\n",
    "        # Evaluation\n",
    "        if epoch % 1 == 0:\n",
    "                sys.stdout.write('\\rEvaluating Epoch %d...' % epoch)\n",
    "                print(\"\\n\")\n",
    "                if epoch>0:\n",
    "                        rt_test, tp_rest = autoencoder.predict(x_test, batch_size=64)\n",
    "                        mae = mean_absolute_error(rt_test_labels, rt_test)\n",
    "                        rmse = np.sqrt(mean_squared_error(rt_test_labels, rt_test))\n",
    "                        mae_tp = mean_absolute_error(tp_test_labels, tp_rest)\n",
    "                        rmse_tp = np.sqrt(mean_squared_error(tp_test_labels, tp_rest))\n",
    "                        loss = history.history['loss'][0]\n",
    "                        sys.stdout.write('\\rEvaluating completes.[%.1fs] ' % (time() - end))\n",
    "                        if mae + rmse < best_mae + best_rmse:\n",
    "                                best_mae, best_rmse, best_epoch = mae, rmse, epoch\n",
    "                        evalResults[epoch, :] = [mae, rmse,loss]\n",
    "                        sys.stdout.write('\\rEpoch %d : MAE = %.4f|RMSE = %.4f|MAE_tp = %.4f|RMSE_tp = %.4f|Loss = %.4f\\n' % (epoch, mae, rmse,mae_tp,rmse_tp, loss))\n",
    "print('=' * 14 + 'Training Complete!' + '=' * 18)\n",
    "print('The best is at epoch %d : MAE = %.4f|RMSE = %.4f.' % (best_epoch, best_mae, best_rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
